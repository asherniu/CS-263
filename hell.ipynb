{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "from techniques import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lemmatizer = WordNetLemmatizer() # set lemmatizer\n",
    "stemmer = PorterStemmer() # set stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 1193514  words loaded!\n"
     ]
    }
   ],
   "source": [
    "### Glove related. Function to load a glove file. You need to download it. \n",
    "def loadGloveModel(gloveFile):\n",
    "    '''\n",
    "    Load a Glove file. Please download it from http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "    '''\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf-8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "# Load the model. Change the path if needed\n",
    "model = loadGloveModel('glove.twitter.27B.200d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to generate an average of Glove to represent OOV (out-of-vocabulary) words\n",
    "#### The actual codes are commented out to save time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generate average vec start\n",
    "# GLOVE_FILE = 'glove.twitter.27B.200d.txt'\n",
    "# # Get number of vectors and hidden dim\n",
    "# with open(GLOVE_FILE, 'r',encoding=\"utf8\") as f:\n",
    "#     for i, line in tqdm(enumerate(f)):\n",
    "#         pass\n",
    "# n_vec = i + 1\n",
    "# hidden_dim = len(line.split(' ')) - 1\n",
    "\n",
    "# vecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)\n",
    "\n",
    "# with open(GLOVE_FILE, 'r',encoding=\"utf8\") as f:\n",
    "#     for i, line in tqdm(enumerate(f)):\n",
    "#         vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
    "\n",
    "# oov = np.mean(vecs, axis=0)\n",
    "# ## Generate average vec end\n",
    "\n",
    "oov = np.array([-0.0940983 , -0.02843253, -0.01981537,  0.00476221,  0.00562878,\n",
    "       -0.08310315, -0.10455726, -0.11054859,  0.04218907, -0.05685968,\n",
    "       -0.07577542,  0.01502523,  0.12450685, -0.03634054,  0.0405103 ,\n",
    "        0.01487466,  0.03977518,  0.04460221,  0.09106229,  0.05568037,\n",
    "        0.02800928,  0.06613768, -0.01108837,  0.01697896, -0.10412963,\n",
    "        0.3507236 ,  0.02584417,  0.00360701, -0.18075785,  0.00826703,\n",
    "        0.15338054,  0.09718955,  0.18591996, -0.12195268, -0.19989698,\n",
    "       -0.12006074,  0.0120836 ,  0.01818978,  0.10259497,  0.02875553,\n",
    "        0.39889172, -0.00370644,  0.03968571, -0.041596  , -0.13434792,\n",
    "        0.10075227, -0.05748982,  0.00397006,  0.08035477,  0.12797299,\n",
    "        0.08266383,  0.05858067, -0.04357374, -0.08068992, -0.04668539,\n",
    "       -0.06677692,  0.04721875,  0.00727302, -0.05912784, -0.04276676,\n",
    "        0.09697088,  0.04666249,  0.02529906,  0.04875451, -0.17676166,\n",
    "        0.19386147,  0.12038179,  0.08816861, -0.0571619 , -0.00090087,\n",
    "       -0.11305717,  0.05075013, -0.05707799, -0.03146713,  0.03141678,\n",
    "        0.14196746,  0.09224427, -0.01728742,  0.01046179,  0.03585972,\n",
    "       -0.09386539, -0.09762925, -0.06350451, -0.00260555, -0.0661344 ,\n",
    "       -0.0807054 ,  0.1642215 , -0.2894754 ,  0.00306663, -0.05767346,\n",
    "        0.00879237,  0.12618215,  0.05366566,  0.07843259,  0.03640802,\n",
    "       -0.00649674,  0.01998443,  0.06910556, -0.13995245,  0.04371282,\n",
    "       -0.06699623, -0.00429413, -0.09469022, -0.09512661, -0.12079419,\n",
    "       -0.1227353 ,  0.11687743, -0.01618212,  0.13010803, -0.07520641,\n",
    "        0.08611171, -0.11750808, -0.03315089,  0.03394032, -0.04863394,\n",
    "       -0.04742027,  0.03740013,  0.05302504,  0.0292228 ,  0.00939865,\n",
    "       -0.02425062,  0.00254103,  0.04668314, -0.02513516,  0.01587607,\n",
    "        0.13379093, -0.02098919, -0.02138812,  0.12004584, -0.12496009,\n",
    "        0.09164985,  0.02535828,  0.030993  ,  0.01908904, -0.1135054 ,\n",
    "       -0.05747874, -0.00097564, -0.07191793, -0.04650274,  0.09658901,\n",
    "       -0.03481024,  0.07673627,  0.07474707, -0.12794939, -0.03416433,\n",
    "       -0.125318  ,  0.05369347, -0.08551957,  0.00373768,  0.01549865,\n",
    "        0.11278545, -0.0193907 ,  0.8663707 , -0.09132162, -0.04578998,\n",
    "        0.08276154, -0.05723441, -0.04189969,  0.04966758,  0.0884276 ,\n",
    "        0.07552473, -0.08842846, -0.03441407,  0.02968801,  0.03326621,\n",
    "       -0.01641093,  0.02045972, -0.04570299, -0.04881294,  0.03214714,\n",
    "        0.08044318,  0.05654246, -0.0202872 , -0.06148338,  0.029538  ,\n",
    "       -0.12749897, -0.09602174, -0.09560092,  0.08549695, -0.081919  ,\n",
    "       -0.06052832,  0.01137542, -0.01080422, -0.08540731,  0.00334476,\n",
    "        0.18626893,  0.13344373,  0.0643768 , -0.06667358,  0.04245436,\n",
    "        0.10244108,  0.00752079,  0.06437495,  0.05950141,  0.11263546,\n",
    "       -0.01919787,  0.255039  ,  0.14020036,  0.08237398, -0.2602364 ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Codes are from https://github.com/Deffro/text-preprocessing-techniques\n",
    "\n",
    "\"\"\" Tokenizes a text to its words, removes and replaces some of them \"\"\"    \n",
    "finalTokens = [] # all tokens\n",
    "stoplist = stopwords.words('english')\n",
    "my_stopwords = \"url atuser st rd nd th am pm atus\" # my extra stopwords\n",
    "stoplist = stoplist + my_stopwords.split()\n",
    "allowedWordTypes = [\"J\",\"R\",\"V\",\"N\"] #  J is Adject, R is Adverb, V is Verb, N is Noun. These are used for POS Tagging\n",
    "\n",
    "def tokenize(text, wordCountBefore, textID, y, file):\n",
    "    totalAdjectives = 0\n",
    "    totalAdverbs = 0\n",
    "    totalVerbs = 0\n",
    "    onlyOneSentenceTokens = [] # tokens of one sentence each time\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    tokens = replaceNegations(tokens) # Technique 6: finds \"not\" and antonym for the next word and if found, replaces not and the next word with the antonym\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator) # Technique 7: remove punctuation\n",
    "\n",
    "    tokens = nltk.word_tokenize(text) # it takes a text as an input and provides a list of every token in it\n",
    "\n",
    "#     ### POS TAGGING BEGIN (If you want to exclude words using POS Tagging, keep this section uncommented and comment the above) ###          \n",
    "    tagged = nltk.pos_tag(tokens) # Technique 13: part of speech tagging  \n",
    "    for w in tagged:\n",
    "\n",
    "        if (w[1][0] in allowedWordTypes and w[0] not in stoplist):\n",
    "            final_word = addCapTag(w[0])\n",
    "            #final_word = final_word.lower()\n",
    "            final_word = replaceElongated(final_word)\n",
    "            if len(final_word)>1:\n",
    "                final_word = spellCorrection(final_word)\n",
    "#             final_word = lemmatizer.lemmatize(final_word)\n",
    "#             final_word = stemmer.stem(final_word)\n",
    "\n",
    "#     # ### POS TAGGING END ###\n",
    "## NO POS TAGGING BEGIN (If you don't want to use POS Tagging keep this section uncommented) ###\n",
    "\n",
    "#     for w in tokens:\n",
    "#         if (w not in stoplist): # Technique 10: remove stopwords\n",
    "#             final_word = addCapTag(w) # Technique 8: Finds a word with at least 3 characters capitalized and adds the tag ALL_CAPS_\n",
    "#             final_word = final_word.lower() # Technique 9: lowercases all characters\n",
    "#             final_word = replaceElongated(final_word) # Technique 11: replaces an elongated word with its basic form, unless the word exists in the lexicon\n",
    "#             if len(final_word)>1:\n",
    "#                 final_word = spellCorrection(final_word) # Technique 12: correction of spelling errors\n",
    "#             final_word = lemmatizer.lemmatize(final_word) # Technique 14: lemmatizes words\n",
    "#             final_word = stemmer.stem(final_word) # Technique 15: apply stemming to words\n",
    "\n",
    "## NO POS TAGGING END ###\n",
    "                \n",
    "            onlyOneSentenceTokens.append(final_word)           \n",
    "            finalTokens.append(final_word)\n",
    "\n",
    "         \n",
    "    onlyOneSentence = \" \".join(onlyOneSentenceTokens) # form again the sentence from the list of tokens\n",
    "    #print(onlyOneSentence) # print final sentence\n",
    "\n",
    "    \n",
    "    \"\"\" Write the preprocessed text to file \"\"\"\n",
    "    with open(file, \"a\") as result:\n",
    "        result.write(textID+\"\\t\"+y+\"\\t\"+onlyOneSentence+\"\\n\")\n",
    "        \n",
    "    return finalTokens\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block of codes to tokenize text data. The result does not have the features, only tokens. You can ignore it since I have also included the final output in train.csv and test.csv. \n",
    "### So the input for this is raw data, let's name the output A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aec0f19dc56434aa07a5e3a8dede76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc0adfc61e54ee782b1e6c24bc66858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20631), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# f1 = open(\"twitter-2016train-A.txt\",\"r\", encoding=\"utf8\", errors='replace').read()\n",
    "\n",
    "# t0 = time()\n",
    "# totalSentences = 0\n",
    "# totalEmoticons = 0\n",
    "# totalSlangs = 0\n",
    "# totalSlangsFound = []\n",
    "# totalElongated = 0\n",
    "# totalMultiExclamationMarks = 0\n",
    "# totalMultiQuestionMarks = 0\n",
    "# totalMultiStopMarks = 0\n",
    "# totalAllCaps = 0\n",
    "\n",
    "# for line in tqdm(f1.split('\\n')):\n",
    "#     totalSentences += 1\n",
    "#     feat = []\n",
    "#     columns = line.split('\\t')\n",
    "#     columns = [col.strip() for col in columns]\n",
    "\n",
    "#     textID = (columns[0])\n",
    "#     y = (columns[1])\n",
    "\n",
    "#     text = removeUnicode(columns[2]) # Technique 0\n",
    "#     #print(text) # print initial text\n",
    "#     wordCountBefore = len(re.findall(r'\\w+', text)) # word count of one sentence before preprocess    \n",
    "#     #print(\"Words before preprocess: \",wordCountBefore,\"\\n\")\n",
    "    \n",
    "#     text = replaceURL(text) # Technique 1\n",
    "#     text = replaceAtUser(text) # Technique 1\n",
    "#     text = removeHashtagInFrontOfWord(text) # Technique 1\n",
    "\n",
    "#     temp_slangs, temp_slangsFound = countSlang(text)\n",
    "# #     totalSlangs += temp_slangs # total slangs for all sentences\n",
    "#     for word in temp_slangsFound:\n",
    "#         totalSlangsFound.append(word) # all the slangs found in all sentences\n",
    "    \n",
    "#     text = replaceSlang(text) # Technique 2: replaces slang words and abbreviations with their equivalents\n",
    "#     text = replaceContraction(text) # Technique 3: replaces contractions to their equivalents\n",
    "#     text = removeNumbers(text) # Technique 4: remove integers from text\n",
    "\n",
    "# #     emoticons = countEmoticons(text) # how many emoticons in this sentence\n",
    "# #     totalEmoticons += emoticons\n",
    "    \n",
    "#     text = removeEmoticons(text) # removes emoticons from text\n",
    "\n",
    "    \n",
    "# #     totalAllCaps += countAllCaps(text)\n",
    "\n",
    "#     totalMultiExclamationMarks += countMultiExclamationMarks(text) # how many repetitions of exlamation marks in this sentence\n",
    "#     totalMultiQuestionMarks += countMultiQuestionMarks(text) # how many repetitions of question marks in this sentence\n",
    "#     totalMultiStopMarks += countMultiStopMarks(text) # how many repetitions of stop marks in this sentence\n",
    "    \n",
    "#     text = replaceMultiExclamationMark(text) # Technique 5: replaces repetitions of exlamation\n",
    "#     text = replaceMultiQuestionMark(text) # Technique 5: replaces repetitions of question marks \n",
    "#     text = replaceMultiStopMark(text) # Technique 5: replaces repetitions of stop marks \n",
    "#     totalElongated += countElongated(text) # how many elongated words emoticons in this sentence\n",
    "    \n",
    "#     tokens = tokenize(text, wordCountBefore, textID, y, 'result-2016train-A.txt')  \n",
    "\n",
    "# f2 = open(\"twitter-2016test-A.txt\",\"r\", encoding=\"utf8\", errors='replace').read()\n",
    " \n",
    "# for line in tqdm(f2.split('\\n')):\n",
    "#     totalSentences += 1\n",
    "#     feat = []\n",
    "#     columns = line.split('\\t')\n",
    "#     columns = [col.strip() for col in columns]\n",
    "\n",
    "#     textID = (columns[0])\n",
    "#     y = (columns[1])\n",
    "\n",
    "#     text = removeUnicode(columns[2]) # Technique 0\n",
    "#     #print(text) # print initial text\n",
    "#     wordCountBefore = len(re.findall(r'\\w+', text)) # word count of one sentence before preprocess    \n",
    "#     #print(\"Words before preprocess: \",wordCountBefore,\"\\n\")\n",
    "    \n",
    "#     text = replaceURL(text) # Technique 1\n",
    "#     text = replaceAtUser(text) # Technique 1\n",
    "#     text = removeHashtagInFrontOfWord(text) # Technique 1\n",
    "\n",
    "#     temp_slangs, temp_slangsFound = countSlang(text)\n",
    "# #     totalSlangs += temp_slangs # total slangs for all sentences\n",
    "#     for word in temp_slangsFound:\n",
    "#         totalSlangsFound.append(word) # all the slangs found in all sentences\n",
    "    \n",
    "#     text = replaceSlang(text) # Technique 2: replaces slang words and abbreviations with their equivalents\n",
    "#     text = replaceContraction(text) # Technique 3: replaces contractions to their equivalents\n",
    "#     text = removeNumbers(text) # Technique 4: remove integers from text\n",
    "\n",
    "# #     emoticons = countEmoticons(text) # how many emoticons in this sentence\n",
    "# #     totalEmoticons += emoticons\n",
    "    \n",
    "#     text = removeEmoticons(text) # removes emoticons from text\n",
    "\n",
    "    \n",
    "# #     totalAllCaps += countAllCaps(text)\n",
    "\n",
    "#     totalMultiExclamationMarks += countMultiExclamationMarks(text) # how many repetitions of exlamation marks in this sentence\n",
    "#     totalMultiQuestionMarks += countMultiQuestionMarks(text) # how many repetitions of question marks in this sentence\n",
    "#     totalMultiStopMarks += countMultiStopMarks(text) # how many repetitions of stop marks in this sentence\n",
    "    \n",
    "#     text = replaceMultiExclamationMark(text) # Technique 5: replaces repetitions of exlamation\n",
    "#     text = replaceMultiQuestionMark(text) # Technique 5: replaces repetitions of question marks \n",
    "#     text = replaceMultiStopMark(text) # Technique 5: replaces repetitions of stop marks \n",
    "#     totalElongated += countElongated(text) # how many elongated words emoticons in this sentence\n",
    "    \n",
    "#     tokens = tokenize(text, wordCountBefore, textID, y, 'result-2016test-A.txt')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['result-2016test-A', 'result-2016train-A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block of codes to generate features.  You can ignore it since I have also included the final output in train.csv and test.csv\n",
    "### The input of this block is A, let's name the output B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06483c7b63294e158e8d426f339d4395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbb4138d0484a5681a799c8a2e3a6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for file in files:\n",
    "#     data = pd.read_csv(file+'.txt', sep='\\t',header = None, encoding='latin-1')\n",
    "#     data.columns = ['id', 'label', 'tokens']\n",
    "#     data.loc[data['label'] == 'positive', 'label']=1\n",
    "#     data.loc[data['label'] == 'neutral', 'label']=0\n",
    "#     data.loc[data['label'] == 'negative', 'label']=-1\n",
    "#     for i, row in tqdm(data.iterrows()):\n",
    "#         token = row['tokens'].split(' ')\n",
    "#         feature = []\n",
    "#         for word in token :\n",
    "#             # Use oov if word is not in model\n",
    "#             if (word not in model):\n",
    "#                 feature.append( oov)\n",
    "#             else:\n",
    "#                 feature.append(model[word])\n",
    "#         series = pd.Series(np.average(feature, axis = 0))\n",
    "#         for index, x in series.items():\n",
    "#             data.loc[i,'feat_'+str(index)] = x \n",
    "#     data.to_csv(file + '.csv', encoding='latin-1', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block of codes to process data generated from the tokens.  You can ignore it since I have also included the final output in train.csv and test.csv\n",
    "### The input of this block is B, let's name the output C, which are train.csv and test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in data into panda dataframe. \n",
    "# test = pd.read_csv('result-2016test-A.txt', sep='\\t',header = None,  encoding='latin-1')\n",
    "# test.columns = ['id', 'label', 'tokens']\n",
    "# # Read in data into panda dataframe. \n",
    "# train = pd.read_csv('result-2016train-A.txt', sep='\\t',header = None,  encoding='latin-1')\n",
    "# train.columns = ['id', 'label', 'tokens']\n",
    "# # Change the labels from String to Integer\n",
    "# test.loc[test['label'] == 'positive', 'label']=1\n",
    "# test.loc[test['label'] == 'neutral', 'label']=0\n",
    "# test.loc[test['label'] == 'negative', 'label']=-1\n",
    "# # Change the labels from String to Integer\n",
    "# train.loc[train['label'] == 'positive', 'label']=1\n",
    "# train.loc[train['label'] == 'neutral', 'label']=0\n",
    "# train.loc[train['label'] == 'negative', 'label']=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('result-2016train-A.csv',  encoding='latin-1')\n",
    "test = pd.read_csv('result-2016test-A.csv',  encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[train.columns[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[train.columns[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6295"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5046774271727013"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(10,), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(10,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6868333333333333"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46774271727012745"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
