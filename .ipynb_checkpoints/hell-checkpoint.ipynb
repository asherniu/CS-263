{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\tqdm\\autonotebook.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "from techniques import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lemmatizer = WordNetLemmatizer() # set lemmatizer\n",
    "stemmer = PorterStemmer() # set stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 1193514  words loaded!\n"
     ]
    }
   ],
   "source": [
    "### Glove related. Function to load a glove file. You need to download it. \n",
    "def loadGloveModel(gloveFile):\n",
    "    '''\n",
    "    Load a Glove file. Please download it from http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "    '''\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf-8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "# Load the model. Change the path if needed\n",
    "model = loadGloveModel('glove.twitter.27B.200d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to generate an average of Glove to represent OOV (out-of-vocabulary) words\n",
    "#### The actual codes are commented out to save time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generate average vec start\n",
    "# GLOVE_FILE = 'glove.twitter.27B.200d.txt'\n",
    "# # Get number of vectors and hidden dim\n",
    "# with open(GLOVE_FILE, 'r',encoding=\"utf8\") as f:\n",
    "#     for i, line in tqdm(enumerate(f)):\n",
    "#         pass\n",
    "# n_vec = i + 1\n",
    "# hidden_dim = len(line.split(' ')) - 1\n",
    "\n",
    "# vecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)\n",
    "\n",
    "# with open(GLOVE_FILE, 'r',encoding=\"utf8\") as f:\n",
    "#     for i, line in tqdm(enumerate(f)):\n",
    "#         vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
    "\n",
    "# oov = np.mean(vecs, axis=0)\n",
    "# ## Generate average vec end\n",
    "\n",
    "oov = np.array([-0.0940983 , -0.02843253, -0.01981537,  0.00476221,  0.00562878,\n",
    "       -0.08310315, -0.10455726, -0.11054859,  0.04218907, -0.05685968,\n",
    "       -0.07577542,  0.01502523,  0.12450685, -0.03634054,  0.0405103 ,\n",
    "        0.01487466,  0.03977518,  0.04460221,  0.09106229,  0.05568037,\n",
    "        0.02800928,  0.06613768, -0.01108837,  0.01697896, -0.10412963,\n",
    "        0.3507236 ,  0.02584417,  0.00360701, -0.18075785,  0.00826703,\n",
    "        0.15338054,  0.09718955,  0.18591996, -0.12195268, -0.19989698,\n",
    "       -0.12006074,  0.0120836 ,  0.01818978,  0.10259497,  0.02875553,\n",
    "        0.39889172, -0.00370644,  0.03968571, -0.041596  , -0.13434792,\n",
    "        0.10075227, -0.05748982,  0.00397006,  0.08035477,  0.12797299,\n",
    "        0.08266383,  0.05858067, -0.04357374, -0.08068992, -0.04668539,\n",
    "       -0.06677692,  0.04721875,  0.00727302, -0.05912784, -0.04276676,\n",
    "        0.09697088,  0.04666249,  0.02529906,  0.04875451, -0.17676166,\n",
    "        0.19386147,  0.12038179,  0.08816861, -0.0571619 , -0.00090087,\n",
    "       -0.11305717,  0.05075013, -0.05707799, -0.03146713,  0.03141678,\n",
    "        0.14196746,  0.09224427, -0.01728742,  0.01046179,  0.03585972,\n",
    "       -0.09386539, -0.09762925, -0.06350451, -0.00260555, -0.0661344 ,\n",
    "       -0.0807054 ,  0.1642215 , -0.2894754 ,  0.00306663, -0.05767346,\n",
    "        0.00879237,  0.12618215,  0.05366566,  0.07843259,  0.03640802,\n",
    "       -0.00649674,  0.01998443,  0.06910556, -0.13995245,  0.04371282,\n",
    "       -0.06699623, -0.00429413, -0.09469022, -0.09512661, -0.12079419,\n",
    "       -0.1227353 ,  0.11687743, -0.01618212,  0.13010803, -0.07520641,\n",
    "        0.08611171, -0.11750808, -0.03315089,  0.03394032, -0.04863394,\n",
    "       -0.04742027,  0.03740013,  0.05302504,  0.0292228 ,  0.00939865,\n",
    "       -0.02425062,  0.00254103,  0.04668314, -0.02513516,  0.01587607,\n",
    "        0.13379093, -0.02098919, -0.02138812,  0.12004584, -0.12496009,\n",
    "        0.09164985,  0.02535828,  0.030993  ,  0.01908904, -0.1135054 ,\n",
    "       -0.05747874, -0.00097564, -0.07191793, -0.04650274,  0.09658901,\n",
    "       -0.03481024,  0.07673627,  0.07474707, -0.12794939, -0.03416433,\n",
    "       -0.125318  ,  0.05369347, -0.08551957,  0.00373768,  0.01549865,\n",
    "        0.11278545, -0.0193907 ,  0.8663707 , -0.09132162, -0.04578998,\n",
    "        0.08276154, -0.05723441, -0.04189969,  0.04966758,  0.0884276 ,\n",
    "        0.07552473, -0.08842846, -0.03441407,  0.02968801,  0.03326621,\n",
    "       -0.01641093,  0.02045972, -0.04570299, -0.04881294,  0.03214714,\n",
    "        0.08044318,  0.05654246, -0.0202872 , -0.06148338,  0.029538  ,\n",
    "       -0.12749897, -0.09602174, -0.09560092,  0.08549695, -0.081919  ,\n",
    "       -0.06052832,  0.01137542, -0.01080422, -0.08540731,  0.00334476,\n",
    "        0.18626893,  0.13344373,  0.0643768 , -0.06667358,  0.04245436,\n",
    "        0.10244108,  0.00752079,  0.06437495,  0.05950141,  0.11263546,\n",
    "       -0.01919787,  0.255039  ,  0.14020036,  0.08237398, -0.2602364 ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov = np.zeros(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Codes are from https://github.com/Deffro/text-preprocessing-techniques\n",
    "\n",
    "\"\"\" Tokenizes a text to its words, removes and replaces some of them \"\"\"    \n",
    "finalTokens = [] # all tokens\n",
    "stoplist = stopwords.words('english')\n",
    "my_stopwords = \"url atuser st rd nd th am pm atus\" # my extra stopwords\n",
    "stoplist = stoplist + my_stopwords.split()\n",
    "allowedWordTypes = [\"J\",\"R\",\"V\",\"N\"] #  J is Adject, R is Adverb, V is Verb, N is Noun. These are used for POS Tagging\n",
    "\n",
    "def tokenize(text, wordCountBefore, textID, y, file):\n",
    "    totalAdjectives = 0\n",
    "    totalAdverbs = 0\n",
    "    totalVerbs = 0\n",
    "    onlyOneSentenceTokens = [] # tokens of one sentence each time\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    tokens = replaceNegations(tokens) # Technique 6: finds \"not\" and antonym for the next word and if found, replaces not and the next word with the antonym\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator) # Technique 7: remove punctuation\n",
    "\n",
    "    tokens = nltk.word_tokenize(text) # it takes a text as an input and provides a list of every token in it\n",
    "\n",
    "#     ### POS TAGGING BEGIN (If you want to exclude words using POS Tagging, keep this section uncommented and comment the above) ###          \n",
    "    tagged = nltk.pos_tag(tokens) # Technique 13: part of speech tagging  \n",
    "    for w in tagged:\n",
    "\n",
    "        if (w[1][0] in allowedWordTypes and w[0] not in stoplist):\n",
    "            final_word = addCapTag(w[0])\n",
    "            #final_word = final_word.lower()\n",
    "            final_word = replaceElongated(final_word)\n",
    "            if len(final_word)>1:\n",
    "                final_word = spellCorrection(final_word)\n",
    "#             final_word = lemmatizer.lemmatize(final_word)\n",
    "#             final_word = stemmer.stem(final_word)\n",
    "\n",
    "#     # ### POS TAGGING END ###\n",
    "## NO POS TAGGING BEGIN (If you don't want to use POS Tagging keep this section uncommented) ###\n",
    "\n",
    "#     for w in tokens:\n",
    "#         if (w not in stoplist): # Technique 10: remove stopwords\n",
    "#             final_word = addCapTag(w) # Technique 8: Finds a word with at least 3 characters capitalized and adds the tag ALL_CAPS_\n",
    "#             final_word = final_word.lower() # Technique 9: lowercases all characters\n",
    "#             final_word = replaceElongated(final_word) # Technique 11: replaces an elongated word with its basic form, unless the word exists in the lexicon\n",
    "#             if len(final_word)>1:\n",
    "#                 final_word = spellCorrection(final_word) # Technique 12: correction of spelling errors\n",
    "#             final_word = lemmatizer.lemmatize(final_word) # Technique 14: lemmatizes words\n",
    "#             final_word = stemmer.stem(final_word) # Technique 15: apply stemming to words\n",
    "\n",
    "## NO POS TAGGING END ###\n",
    "                \n",
    "            onlyOneSentenceTokens.append(final_word)           \n",
    "            finalTokens.append(final_word)\n",
    "\n",
    "         \n",
    "    onlyOneSentence = \" \".join(onlyOneSentenceTokens) # form again the sentence from the list of tokens\n",
    "    #print(onlyOneSentence) # print final sentence\n",
    "\n",
    "    \n",
    "    \"\"\" Write the preprocessed text to file \"\"\"\n",
    "    with open(file, \"a\") as result:\n",
    "        result.write(textID+\"\\t\"+y+\"\\t\"+onlyOneSentence+\"\\n\")\n",
    "        \n",
    "    return finalTokens\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block of codes to tokenize text data. The result does not have the features, only tokens. You can ignore it since I have also included the final output in train.csv and test.csv. \n",
    "### So the input for this is raw data, let's name the output A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aec0f19dc56434aa07a5e3a8dede76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc0adfc61e54ee782b1e6c24bc66858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20631), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# f1 = open(\"twitter-2016train-A.txt\",\"r\", encoding=\"utf8\", errors='replace').read()\n",
    "\n",
    "# t0 = time()\n",
    "# totalSentences = 0\n",
    "# totalEmoticons = 0\n",
    "# totalSlangs = 0\n",
    "# totalSlangsFound = []\n",
    "# totalElongated = 0\n",
    "# totalMultiExclamationMarks = 0\n",
    "# totalMultiQuestionMarks = 0\n",
    "# totalMultiStopMarks = 0\n",
    "# totalAllCaps = 0\n",
    "\n",
    "# for line in tqdm(f1.split('\\n')):\n",
    "#     totalSentences += 1\n",
    "#     feat = []\n",
    "#     columns = line.split('\\t')\n",
    "#     columns = [col.strip() for col in columns]\n",
    "\n",
    "#     textID = (columns[0])\n",
    "#     y = (columns[1])\n",
    "\n",
    "#     text = removeUnicode(columns[2]) # Technique 0\n",
    "#     #print(text) # print initial text\n",
    "#     wordCountBefore = len(re.findall(r'\\w+', text)) # word count of one sentence before preprocess    \n",
    "#     #print(\"Words before preprocess: \",wordCountBefore,\"\\n\")\n",
    "    \n",
    "#     text = replaceURL(text) # Technique 1\n",
    "#     text = replaceAtUser(text) # Technique 1\n",
    "#     text = removeHashtagInFrontOfWord(text) # Technique 1\n",
    "\n",
    "#     temp_slangs, temp_slangsFound = countSlang(text)\n",
    "# #     totalSlangs += temp_slangs # total slangs for all sentences\n",
    "#     for word in temp_slangsFound:\n",
    "#         totalSlangsFound.append(word) # all the slangs found in all sentences\n",
    "    \n",
    "#     text = replaceSlang(text) # Technique 2: replaces slang words and abbreviations with their equivalents\n",
    "#     text = replaceContraction(text) # Technique 3: replaces contractions to their equivalents\n",
    "#     text = removeNumbers(text) # Technique 4: remove integers from text\n",
    "\n",
    "# #     emoticons = countEmoticons(text) # how many emoticons in this sentence\n",
    "# #     totalEmoticons += emoticons\n",
    "    \n",
    "#     text = removeEmoticons(text) # removes emoticons from text\n",
    "\n",
    "    \n",
    "# #     totalAllCaps += countAllCaps(text)\n",
    "\n",
    "#     totalMultiExclamationMarks += countMultiExclamationMarks(text) # how many repetitions of exlamation marks in this sentence\n",
    "#     totalMultiQuestionMarks += countMultiQuestionMarks(text) # how many repetitions of question marks in this sentence\n",
    "#     totalMultiStopMarks += countMultiStopMarks(text) # how many repetitions of stop marks in this sentence\n",
    "    \n",
    "#     text = replaceMultiExclamationMark(text) # Technique 5: replaces repetitions of exlamation\n",
    "#     text = replaceMultiQuestionMark(text) # Technique 5: replaces repetitions of question marks \n",
    "#     text = replaceMultiStopMark(text) # Technique 5: replaces repetitions of stop marks \n",
    "#     totalElongated += countElongated(text) # how many elongated words emoticons in this sentence\n",
    "    \n",
    "#     tokens = tokenize(text, wordCountBefore, textID, y, 'result-2016train-A.txt')  \n",
    "\n",
    "# f2 = open(\"twitter-2016test-A.txt\",\"r\", encoding=\"utf8\", errors='replace').read()\n",
    " \n",
    "# for line in tqdm(f2.split('\\n')):\n",
    "#     totalSentences += 1\n",
    "#     feat = []\n",
    "#     columns = line.split('\\t')\n",
    "#     columns = [col.strip() for col in columns]\n",
    "\n",
    "#     textID = (columns[0])\n",
    "#     y = (columns[1])\n",
    "\n",
    "#     text = removeUnicode(columns[2]) # Technique 0\n",
    "#     #print(text) # print initial text\n",
    "#     wordCountBefore = len(re.findall(r'\\w+', text)) # word count of one sentence before preprocess    \n",
    "#     #print(\"Words before preprocess: \",wordCountBefore,\"\\n\")\n",
    "    \n",
    "#     text = replaceURL(text) # Technique 1\n",
    "#     text = replaceAtUser(text) # Technique 1\n",
    "#     text = removeHashtagInFrontOfWord(text) # Technique 1\n",
    "\n",
    "#     temp_slangs, temp_slangsFound = countSlang(text)\n",
    "# #     totalSlangs += temp_slangs # total slangs for all sentences\n",
    "#     for word in temp_slangsFound:\n",
    "#         totalSlangsFound.append(word) # all the slangs found in all sentences\n",
    "    \n",
    "#     text = replaceSlang(text) # Technique 2: replaces slang words and abbreviations with their equivalents\n",
    "#     text = replaceContraction(text) # Technique 3: replaces contractions to their equivalents\n",
    "#     text = removeNumbers(text) # Technique 4: remove integers from text\n",
    "\n",
    "# #     emoticons = countEmoticons(text) # how many emoticons in this sentence\n",
    "# #     totalEmoticons += emoticons\n",
    "    \n",
    "#     text = removeEmoticons(text) # removes emoticons from text\n",
    "\n",
    "    \n",
    "# #     totalAllCaps += countAllCaps(text)\n",
    "\n",
    "#     totalMultiExclamationMarks += countMultiExclamationMarks(text) # how many repetitions of exlamation marks in this sentence\n",
    "#     totalMultiQuestionMarks += countMultiQuestionMarks(text) # how many repetitions of question marks in this sentence\n",
    "#     totalMultiStopMarks += countMultiStopMarks(text) # how many repetitions of stop marks in this sentence\n",
    "    \n",
    "#     text = replaceMultiExclamationMark(text) # Technique 5: replaces repetitions of exlamation\n",
    "#     text = replaceMultiQuestionMark(text) # Technique 5: replaces repetitions of question marks \n",
    "#     text = replaceMultiStopMark(text) # Technique 5: replaces repetitions of stop marks \n",
    "#     totalElongated += countElongated(text) # how many elongated words emoticons in this sentence\n",
    "    \n",
    "#     tokens = tokenize(text, wordCountBefore, textID, y, 'result-2016test-A.txt')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['result-2016test-A', 'result-2016train-A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block of codes to generate features.  You can ignore it since I have also included the final output in train.csv and test.csv\n",
    "### The input of this block is A, let's name the output B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f8ea3baed3481c8cdfee3e102f5f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acfb187dd334f648c817be492268601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    data = pd.read_csv(file+'.txt', sep='\\t',header = None, encoding='latin-1')\n",
    "    data.columns = ['id', 'label', 'tokens']\n",
    "    data.loc[data['label'] == 'positive', 'label']=1\n",
    "    data.loc[data['label'] == 'neutral', 'label']=0\n",
    "    data.loc[data['label'] == 'negative', 'label']=-1\n",
    "    for i, row in tqdm(data.iterrows()):\n",
    "        token = row['tokens'].split(' ')\n",
    "        feature = []\n",
    "        length = 0\n",
    "        for word in token :\n",
    "            # Use oov if word is not in model\n",
    "            if (word in model):\n",
    "                length+=1\n",
    "                feature.append(model[word])\n",
    "        if (length == 0):\n",
    "            series = pd.Series(np.sum(feature, axis = 0)*length)\n",
    "        else:\n",
    "            series = pd.Series(np.sum(feature, axis = 0)/length)\n",
    "        for index, x in series.items():\n",
    "            data.loc[i,'feat_'+str(index)] = x \n",
    "    data.to_csv(file + '.csv', encoding='latin-1', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd91c00cdab43aabf492e5d052a3076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mF:\\Anaconda\\lib\\abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[1;34m(cls, instance)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.lib.c_is_list_like'\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Anaconda\\lib\\abc.py\", line 139, in __instancecheck__\n",
      "    return _abc_instancecheck(cls, instance)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-635dcf8a44b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mseries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'feat_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin-1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    555\u001b[0m                 \u001b[1;31m# scalar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m                     \u001b[0msetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36msetter\u001b[1;34m(item, v)\u001b[0m\n\u001b[0;32m    487\u001b[0m                     \u001b[1;31m# set the item, possibly having a dtype change\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m                     \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m                     \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m                     \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   5994\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5995\u001b[0m         \"\"\"\n\u001b[1;32m-> 5996\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5997\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    786\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 788\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"copy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mas_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m    436\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malign_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m             \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('result-2016test-A'+'.txt', sep='\\t',header = None, encoding='latin-1')\n",
    "data.columns = ['id', 'label', 'tokens']\n",
    "data.loc[data['label'] == 'positive', 'label']=1\n",
    "data.loc[data['label'] == 'neutral', 'label']=0\n",
    "data.loc[data['label'] == 'negative', 'label']=-1\n",
    "for i, row in tqdm(data.iterrows()):\n",
    "    token = row['tokens'].split(' ')\n",
    "    feature = []\n",
    "    length = 0\n",
    "    for word in token :\n",
    "        # Use oov if word is not in model\n",
    "        if (word in model):\n",
    "            length+=1\n",
    "            feature.append(model[word])\n",
    "    if (length == 0):\n",
    "        series = pd.Series(np.sum(feature, axis = 0)*length)\n",
    "    else:\n",
    "        series = pd.Series(np.sum(feature, axis = 0)/length)\n",
    "    for index, x in series.items():\n",
    "        data.loc[i,'feat_'+str(index)] = x \n",
    "    break\n",
    "# data.to_csv(file + '.csv', encoding='latin-1', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block of codes to process data generated from the tokens.  You can ignore it since I have also included the final output in train.csv and test.csv\n",
    "### The input of this block is B, let's name the output C, which are train.csv and test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in data into panda dataframe. \n",
    "# test = pd.read_csv('result-2016test-A.txt', sep='\\t',header = None,  encoding='latin-1')\n",
    "# test.columns = ['id', 'label', 'tokens']\n",
    "# # Read in data into panda dataframe. \n",
    "# train = pd.read_csv('result-2016train-A.txt', sep='\\t',header = None,  encoding='latin-1')\n",
    "# train.columns = ['id', 'label', 'tokens']\n",
    "# # Change the labels from String to Integer\n",
    "# test.loc[test['label'] == 'positive', 'label']=1\n",
    "# test.loc[test['label'] == 'neutral', 'label']=0\n",
    "# test.loc[test['label'] == 'negative', 'label']=-1\n",
    "# # Change the labels from String to Integer\n",
    "# train.loc[train['label'] == 'positive', 'label']=1\n",
    "# train.loc[train['label'] == 'neutral', 'label']=0\n",
    "# train.loc[train['label'] == 'negative', 'label']=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('result-2016train-A.csv',  encoding='latin-1')\n",
    "test = pd.read_csv('result-2016test-A.csv',  encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[train.columns[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[train.columns[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6315"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1925",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1925",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-08cbd7e17c3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1925\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2978\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2979\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2980\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1925"
     ]
    }
   ],
   "source": [
    "X_test[1925]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_190</th>\n",
       "      <th>feat_191</th>\n",
       "      <th>feat_192</th>\n",
       "      <th>feat_193</th>\n",
       "      <th>feat_194</th>\n",
       "      <th>feat_195</th>\n",
       "      <th>feat_196</th>\n",
       "      <th>feat_197</th>\n",
       "      <th>feat_198</th>\n",
       "      <th>feat_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "1925   False    True    True    True    True    True    True    True    True   \n",
       "\n",
       "      feat_9  ...  feat_190  feat_191  feat_192  feat_193  feat_194  feat_195  \\\n",
       "1925    True  ...      True      True      True      True      True      True   \n",
       "\n",
       "      feat_196  feat_197  feat_198  feat_199  \n",
       "1925      True      True      True      True  \n",
       "\n",
       "[1 rows x 200 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['feat_1'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_190</th>\n",
       "      <th>feat_191</th>\n",
       "      <th>feat_192</th>\n",
       "      <th>feat_193</th>\n",
       "      <th>feat_194</th>\n",
       "      <th>feat_195</th>\n",
       "      <th>feat_196</th>\n",
       "      <th>feat_197</th>\n",
       "      <th>feat_198</th>\n",
       "      <th>feat_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20626</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20627</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20628</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20629</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20630</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20631 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0       False   False   False   False   False   False   False   False   False   \n",
       "1       False   False   False   False   False   False   False   False   False   \n",
       "2       False   False   False   False   False   False   False   False   False   \n",
       "3       False   False   False   False   False   False   False   False   False   \n",
       "4       False   False   False   False   False   False   False   False   False   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "20626   False   False   False   False   False   False   False   False   False   \n",
       "20627   False   False   False   False   False   False   False   False   False   \n",
       "20628   False   False   False   False   False   False   False   False   False   \n",
       "20629   False   False   False   False   False   False   False   False   False   \n",
       "20630   False   False   False   False   False   False   False   False   False   \n",
       "\n",
       "       feat_9  ...  feat_190  feat_191  feat_192  feat_193  feat_194  \\\n",
       "0       False  ...     False     False     False     False     False   \n",
       "1       False  ...     False     False     False     False     False   \n",
       "2       False  ...     False     False     False     False     False   \n",
       "3       False  ...     False     False     False     False     False   \n",
       "4       False  ...     False     False     False     False     False   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "20626   False  ...     False     False     False     False     False   \n",
       "20627   False  ...     False     False     False     False     False   \n",
       "20628   False  ...     False     False     False     False     False   \n",
       "20629   False  ...     False     False     False     False     False   \n",
       "20630   False  ...     False     False     False     False     False   \n",
       "\n",
       "       feat_195  feat_196  feat_197  feat_198  feat_199  \n",
       "0         False     False     False     False     False  \n",
       "1         False     False     False     False     False  \n",
       "2         False     False     False     False     False  \n",
       "3         False     False     False     False     False  \n",
       "4         False     False     False     False     False  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "20626     False     False     False     False     False  \n",
       "20627     False     False     False     False     False  \n",
       "20628     False     False     False     False     False  \n",
       "20629     False     False     False     False     False  \n",
       "20630     False     False     False     False     False  \n",
       "\n",
       "[20631 rows x 200 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-14cf014e97f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    355\u001b[0m         \"\"\"\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \"\"\"\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    263\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 542\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_190</th>\n",
       "      <th>feat_191</th>\n",
       "      <th>feat_192</th>\n",
       "      <th>feat_193</th>\n",
       "      <th>feat_194</th>\n",
       "      <th>feat_195</th>\n",
       "      <th>feat_196</th>\n",
       "      <th>feat_197</th>\n",
       "      <th>feat_198</th>\n",
       "      <th>feat_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.182686</td>\n",
       "      <td>-0.053157</td>\n",
       "      <td>-0.066133</td>\n",
       "      <td>-0.038988</td>\n",
       "      <td>-0.029138</td>\n",
       "      <td>-0.009655</td>\n",
       "      <td>0.351911</td>\n",
       "      <td>-0.110976</td>\n",
       "      <td>-0.083660</td>\n",
       "      <td>-0.242586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137201</td>\n",
       "      <td>0.091585</td>\n",
       "      <td>-0.124332</td>\n",
       "      <td>-0.025468</td>\n",
       "      <td>0.178968</td>\n",
       "      <td>-0.168498</td>\n",
       "      <td>0.120451</td>\n",
       "      <td>0.103347</td>\n",
       "      <td>-0.021166</td>\n",
       "      <td>-0.234675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.095218</td>\n",
       "      <td>0.199759</td>\n",
       "      <td>0.141281</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.238157</td>\n",
       "      <td>-0.116362</td>\n",
       "      <td>0.448471</td>\n",
       "      <td>-0.045370</td>\n",
       "      <td>0.029324</td>\n",
       "      <td>-0.053157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052225</td>\n",
       "      <td>0.135207</td>\n",
       "      <td>0.223978</td>\n",
       "      <td>-0.044857</td>\n",
       "      <td>0.170019</td>\n",
       "      <td>-0.151837</td>\n",
       "      <td>0.039279</td>\n",
       "      <td>0.120695</td>\n",
       "      <td>0.125572</td>\n",
       "      <td>-0.040973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.054332</td>\n",
       "      <td>-0.007380</td>\n",
       "      <td>-0.027974</td>\n",
       "      <td>0.158281</td>\n",
       "      <td>-0.035708</td>\n",
       "      <td>-0.030575</td>\n",
       "      <td>0.609386</td>\n",
       "      <td>-0.070186</td>\n",
       "      <td>-0.287070</td>\n",
       "      <td>-0.151562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007527</td>\n",
       "      <td>0.028636</td>\n",
       "      <td>0.121104</td>\n",
       "      <td>0.048035</td>\n",
       "      <td>-0.080133</td>\n",
       "      <td>0.031808</td>\n",
       "      <td>-0.002123</td>\n",
       "      <td>-0.076050</td>\n",
       "      <td>-0.105424</td>\n",
       "      <td>-0.130276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.201354</td>\n",
       "      <td>0.119068</td>\n",
       "      <td>0.021124</td>\n",
       "      <td>-0.195589</td>\n",
       "      <td>-0.121917</td>\n",
       "      <td>0.141852</td>\n",
       "      <td>0.418118</td>\n",
       "      <td>0.385126</td>\n",
       "      <td>-0.039871</td>\n",
       "      <td>-0.119851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256114</td>\n",
       "      <td>0.277618</td>\n",
       "      <td>-0.090255</td>\n",
       "      <td>-0.106359</td>\n",
       "      <td>-0.289312</td>\n",
       "      <td>0.082443</td>\n",
       "      <td>0.082909</td>\n",
       "      <td>0.039730</td>\n",
       "      <td>-0.032952</td>\n",
       "      <td>-0.078767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.095278</td>\n",
       "      <td>0.157307</td>\n",
       "      <td>0.084263</td>\n",
       "      <td>-0.061007</td>\n",
       "      <td>-0.032520</td>\n",
       "      <td>0.017946</td>\n",
       "      <td>0.376862</td>\n",
       "      <td>0.012014</td>\n",
       "      <td>0.074788</td>\n",
       "      <td>-0.269103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168472</td>\n",
       "      <td>0.011209</td>\n",
       "      <td>0.126717</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>0.061992</td>\n",
       "      <td>-0.131814</td>\n",
       "      <td>0.285412</td>\n",
       "      <td>-0.042320</td>\n",
       "      <td>0.127029</td>\n",
       "      <td>-0.218869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20626</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.298947</td>\n",
       "      <td>0.093476</td>\n",
       "      <td>-0.054900</td>\n",
       "      <td>-0.034671</td>\n",
       "      <td>-0.079568</td>\n",
       "      <td>0.585584</td>\n",
       "      <td>0.006351</td>\n",
       "      <td>-0.072694</td>\n",
       "      <td>-0.088961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170917</td>\n",
       "      <td>0.218123</td>\n",
       "      <td>0.031760</td>\n",
       "      <td>0.011570</td>\n",
       "      <td>0.032519</td>\n",
       "      <td>-0.129225</td>\n",
       "      <td>0.103024</td>\n",
       "      <td>-0.013459</td>\n",
       "      <td>0.095585</td>\n",
       "      <td>-0.115115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20627</td>\n",
       "      <td>-0.105370</td>\n",
       "      <td>-0.024842</td>\n",
       "      <td>0.241625</td>\n",
       "      <td>-0.061957</td>\n",
       "      <td>0.240092</td>\n",
       "      <td>0.156364</td>\n",
       "      <td>0.173678</td>\n",
       "      <td>0.073133</td>\n",
       "      <td>0.197217</td>\n",
       "      <td>0.111540</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114795</td>\n",
       "      <td>-0.060735</td>\n",
       "      <td>0.117210</td>\n",
       "      <td>0.140317</td>\n",
       "      <td>-0.287491</td>\n",
       "      <td>-0.078988</td>\n",
       "      <td>0.221106</td>\n",
       "      <td>0.062376</td>\n",
       "      <td>0.293918</td>\n",
       "      <td>-0.095186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20628</td>\n",
       "      <td>-0.045591</td>\n",
       "      <td>0.141913</td>\n",
       "      <td>-0.041254</td>\n",
       "      <td>0.125390</td>\n",
       "      <td>-0.125325</td>\n",
       "      <td>-0.304552</td>\n",
       "      <td>0.072546</td>\n",
       "      <td>0.041155</td>\n",
       "      <td>0.078924</td>\n",
       "      <td>-0.380926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177209</td>\n",
       "      <td>-0.184647</td>\n",
       "      <td>-0.041628</td>\n",
       "      <td>-0.023597</td>\n",
       "      <td>-0.133942</td>\n",
       "      <td>-0.133093</td>\n",
       "      <td>0.261304</td>\n",
       "      <td>0.113288</td>\n",
       "      <td>0.199995</td>\n",
       "      <td>0.036054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20629</td>\n",
       "      <td>-0.032526</td>\n",
       "      <td>0.137734</td>\n",
       "      <td>0.134703</td>\n",
       "      <td>0.036165</td>\n",
       "      <td>0.025548</td>\n",
       "      <td>0.155885</td>\n",
       "      <td>0.543935</td>\n",
       "      <td>-0.101079</td>\n",
       "      <td>0.058718</td>\n",
       "      <td>-0.032928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137195</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>-0.154881</td>\n",
       "      <td>0.062494</td>\n",
       "      <td>0.024211</td>\n",
       "      <td>-0.164698</td>\n",
       "      <td>0.321154</td>\n",
       "      <td>0.118446</td>\n",
       "      <td>0.214276</td>\n",
       "      <td>0.140274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20630</td>\n",
       "      <td>-0.147583</td>\n",
       "      <td>0.253530</td>\n",
       "      <td>0.186240</td>\n",
       "      <td>0.204284</td>\n",
       "      <td>-0.076384</td>\n",
       "      <td>0.108837</td>\n",
       "      <td>0.386340</td>\n",
       "      <td>0.061353</td>\n",
       "      <td>-0.263643</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077031</td>\n",
       "      <td>-0.082719</td>\n",
       "      <td>-0.060076</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>-0.183250</td>\n",
       "      <td>-0.116000</td>\n",
       "      <td>0.129804</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>-0.214311</td>\n",
       "      <td>-0.206667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20631 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feat_0    feat_1    feat_2    feat_3    feat_4    feat_5    feat_6  \\\n",
       "0      0.182686 -0.053157 -0.066133 -0.038988 -0.029138 -0.009655  0.351911   \n",
       "1     -0.095218  0.199759  0.141281 -0.000017 -0.238157 -0.116362  0.448471   \n",
       "2      0.054332 -0.007380 -0.027974  0.158281 -0.035708 -0.030575  0.609386   \n",
       "3      0.201354  0.119068  0.021124 -0.195589 -0.121917  0.141852  0.418118   \n",
       "4     -0.095278  0.157307  0.084263 -0.061007 -0.032520  0.017946  0.376862   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "20626  0.000423  0.298947  0.093476 -0.054900 -0.034671 -0.079568  0.585584   \n",
       "20627 -0.105370 -0.024842  0.241625 -0.061957  0.240092  0.156364  0.173678   \n",
       "20628 -0.045591  0.141913 -0.041254  0.125390 -0.125325 -0.304552  0.072546   \n",
       "20629 -0.032526  0.137734  0.134703  0.036165  0.025548  0.155885  0.543935   \n",
       "20630 -0.147583  0.253530  0.186240  0.204284 -0.076384  0.108837  0.386340   \n",
       "\n",
       "         feat_7    feat_8    feat_9  ...  feat_190  feat_191  feat_192  \\\n",
       "0     -0.110976 -0.083660 -0.242586  ...  0.137201  0.091585 -0.124332   \n",
       "1     -0.045370  0.029324 -0.053157  ...  0.052225  0.135207  0.223978   \n",
       "2     -0.070186 -0.287070 -0.151562  ...  0.007527  0.028636  0.121104   \n",
       "3      0.385126 -0.039871 -0.119851  ...  0.256114  0.277618 -0.090255   \n",
       "4      0.012014  0.074788 -0.269103  ...  0.168472  0.011209  0.126717   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "20626  0.006351 -0.072694 -0.088961  ...  0.170917  0.218123  0.031760   \n",
       "20627  0.073133  0.197217  0.111540  ... -0.114795 -0.060735  0.117210   \n",
       "20628  0.041155  0.078924 -0.380926  ...  0.177209 -0.184647 -0.041628   \n",
       "20629 -0.101079  0.058718 -0.032928  ... -0.137195  0.006221 -0.154881   \n",
       "20630  0.061353 -0.263643  0.001451  ... -0.077031 -0.082719 -0.060076   \n",
       "\n",
       "       feat_193  feat_194  feat_195  feat_196  feat_197  feat_198  feat_199  \n",
       "0     -0.025468  0.178968 -0.168498  0.120451  0.103347 -0.021166 -0.234675  \n",
       "1     -0.044857  0.170019 -0.151837  0.039279  0.120695  0.125572 -0.040973  \n",
       "2      0.048035 -0.080133  0.031808 -0.002123 -0.076050 -0.105424 -0.130276  \n",
       "3     -0.106359 -0.289312  0.082443  0.082909  0.039730 -0.032952 -0.078767  \n",
       "4      0.002472  0.061992 -0.131814  0.285412 -0.042320  0.127029 -0.218869  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "20626  0.011570  0.032519 -0.129225  0.103024 -0.013459  0.095585 -0.115115  \n",
       "20627  0.140317 -0.287491 -0.078988  0.221106  0.062376  0.293918 -0.095186  \n",
       "20628 -0.023597 -0.133942 -0.133093  0.261304  0.113288  0.199995  0.036054  \n",
       "20629  0.062494  0.024211 -0.164698  0.321154  0.118446  0.214276  0.140274  \n",
       "20630  0.048477 -0.183250 -0.116000  0.129804  0.000162 -0.214311 -0.206667  \n",
       "\n",
       "[20631 rows x 200 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(10,), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(10,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6868333333333333"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46774271727012745"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
