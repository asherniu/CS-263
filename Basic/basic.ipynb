{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\tqdm\\autonotebook.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "from techniques import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() # set lemmatizer\n",
    "stemmer = PorterStemmer() # set stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 1193514  words loaded!\n"
     ]
    }
   ],
   "source": [
    "### Glove related. Function to load a glove file. You need to download it. \n",
    "def loadGloveModel(gloveFile):\n",
    "    '''\n",
    "    Load a Glove file. Please download it from http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "    '''\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf-8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "# Load the model. Change the path if needed\n",
    "model = loadGloveModel('glove.twitter.27B.200d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to generate an average of Glove to represent OOV (out-of-vocabulary) words\n",
    "#### The actual codes are commented out to save time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generate average vec start\n",
    "# GLOVE_FILE = 'glove.twitter.27B.200d.txt'\n",
    "# # Get number of vectors and hidden dim\n",
    "# with open(GLOVE_FILE, 'r',encoding=\"utf8\") as f:\n",
    "#     for i, line in tqdm(enumerate(f)):\n",
    "#         pass\n",
    "# n_vec = i + 1\n",
    "# hidden_dim = len(line.split(' ')) - 1\n",
    "\n",
    "# vecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)\n",
    "\n",
    "# with open(GLOVE_FILE, 'r',encoding=\"utf8\") as f:\n",
    "#     for i, line in tqdm(enumerate(f)):\n",
    "#         vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
    "\n",
    "# oov = np.mean(vecs, axis=0)\n",
    "# ## Generate average vec end\n",
    "\n",
    "oov = np.array([-0.0940983 , -0.02843253, -0.01981537,  0.00476221,  0.00562878,\n",
    "       -0.08310315, -0.10455726, -0.11054859,  0.04218907, -0.05685968,\n",
    "       -0.07577542,  0.01502523,  0.12450685, -0.03634054,  0.0405103 ,\n",
    "        0.01487466,  0.03977518,  0.04460221,  0.09106229,  0.05568037,\n",
    "        0.02800928,  0.06613768, -0.01108837,  0.01697896, -0.10412963,\n",
    "        0.3507236 ,  0.02584417,  0.00360701, -0.18075785,  0.00826703,\n",
    "        0.15338054,  0.09718955,  0.18591996, -0.12195268, -0.19989698,\n",
    "       -0.12006074,  0.0120836 ,  0.01818978,  0.10259497,  0.02875553,\n",
    "        0.39889172, -0.00370644,  0.03968571, -0.041596  , -0.13434792,\n",
    "        0.10075227, -0.05748982,  0.00397006,  0.08035477,  0.12797299,\n",
    "        0.08266383,  0.05858067, -0.04357374, -0.08068992, -0.04668539,\n",
    "       -0.06677692,  0.04721875,  0.00727302, -0.05912784, -0.04276676,\n",
    "        0.09697088,  0.04666249,  0.02529906,  0.04875451, -0.17676166,\n",
    "        0.19386147,  0.12038179,  0.08816861, -0.0571619 , -0.00090087,\n",
    "       -0.11305717,  0.05075013, -0.05707799, -0.03146713,  0.03141678,\n",
    "        0.14196746,  0.09224427, -0.01728742,  0.01046179,  0.03585972,\n",
    "       -0.09386539, -0.09762925, -0.06350451, -0.00260555, -0.0661344 ,\n",
    "       -0.0807054 ,  0.1642215 , -0.2894754 ,  0.00306663, -0.05767346,\n",
    "        0.00879237,  0.12618215,  0.05366566,  0.07843259,  0.03640802,\n",
    "       -0.00649674,  0.01998443,  0.06910556, -0.13995245,  0.04371282,\n",
    "       -0.06699623, -0.00429413, -0.09469022, -0.09512661, -0.12079419,\n",
    "       -0.1227353 ,  0.11687743, -0.01618212,  0.13010803, -0.07520641,\n",
    "        0.08611171, -0.11750808, -0.03315089,  0.03394032, -0.04863394,\n",
    "       -0.04742027,  0.03740013,  0.05302504,  0.0292228 ,  0.00939865,\n",
    "       -0.02425062,  0.00254103,  0.04668314, -0.02513516,  0.01587607,\n",
    "        0.13379093, -0.02098919, -0.02138812,  0.12004584, -0.12496009,\n",
    "        0.09164985,  0.02535828,  0.030993  ,  0.01908904, -0.1135054 ,\n",
    "       -0.05747874, -0.00097564, -0.07191793, -0.04650274,  0.09658901,\n",
    "       -0.03481024,  0.07673627,  0.07474707, -0.12794939, -0.03416433,\n",
    "       -0.125318  ,  0.05369347, -0.08551957,  0.00373768,  0.01549865,\n",
    "        0.11278545, -0.0193907 ,  0.8663707 , -0.09132162, -0.04578998,\n",
    "        0.08276154, -0.05723441, -0.04189969,  0.04966758,  0.0884276 ,\n",
    "        0.07552473, -0.08842846, -0.03441407,  0.02968801,  0.03326621,\n",
    "       -0.01641093,  0.02045972, -0.04570299, -0.04881294,  0.03214714,\n",
    "        0.08044318,  0.05654246, -0.0202872 , -0.06148338,  0.029538  ,\n",
    "       -0.12749897, -0.09602174, -0.09560092,  0.08549695, -0.081919  ,\n",
    "       -0.06052832,  0.01137542, -0.01080422, -0.08540731,  0.00334476,\n",
    "        0.18626893,  0.13344373,  0.0643768 , -0.06667358,  0.04245436,\n",
    "        0.10244108,  0.00752079,  0.06437495,  0.05950141,  0.11263546,\n",
    "       -0.01919787,  0.255039  ,  0.14020036,  0.08237398, -0.2602364 ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Codes are from https://github.com/Deffro/text-preprocessing-techniques\n",
    "\n",
    "\"\"\" Tokenizes a text to its words, removes and replaces some of them \"\"\"    \n",
    "finalTokens = [] # all tokens\n",
    "stoplist = stopwords.words('english')\n",
    "my_stopwords = \"url atuser st rd nd th am pm atus\" # my extra stopwords\n",
    "stoplist = stoplist + my_stopwords.split()\n",
    "allowedWordTypes = [\"J\",\"R\",\"V\",\"N\"] #  J is Adject, R is Adverb, V is Verb, N is Noun. These are used for POS Tagging\n",
    "\n",
    "def tokenize(text, wordCountBefore, textID, y, file):\n",
    "    totalAdjectives = 0\n",
    "    totalAdverbs = 0\n",
    "    totalVerbs = 0\n",
    "    onlyOneSentenceTokens = [] # tokens of one sentence each time\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    tokens = replaceNegations(tokens) # Technique 6: finds \"not\" and antonym for the next word and if found, replaces not and the next word with the antonym\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator) # Technique 7: remove punctuation\n",
    "\n",
    "    tokens = nltk.word_tokenize(text) # it takes a text as an input and provides a list of every token in it\n",
    "\n",
    "#     ### POS TAGGING BEGIN (If you want to exclude words using POS Tagging, keep this section uncommented and comment the above) ###          \n",
    "    tagged = nltk.pos_tag(tokens) # Technique 13: part of speech tagging  \n",
    "    for w in tagged:\n",
    "\n",
    "        if (w[1][0] in allowedWordTypes and w[0] not in stoplist):\n",
    "            final_word = addCapTag(w[0])\n",
    "            #final_word = final_word.lower()\n",
    "            final_word = replaceElongated(final_word)\n",
    "            if len(final_word)>1:\n",
    "                final_word = spellCorrection(final_word)\n",
    "#             final_word = lemmatizer.lemmatize(final_word)\n",
    "#             final_word = stemmer.stem(final_word)\n",
    "\n",
    "#     # ### POS TAGGING END ###\n",
    "## NO POS TAGGING BEGIN (If you don't want to use POS Tagging keep this section uncommented) ###\n",
    "\n",
    "#     for w in tokens:\n",
    "#         if (w not in stoplist): # Technique 10: remove stopwords\n",
    "#             final_word = addCapTag(w) # Technique 8: Finds a word with at least 3 characters capitalized and adds the tag ALL_CAPS_\n",
    "#             final_word = final_word.lower() # Technique 9: lowercases all characters\n",
    "#             final_word = replaceElongated(final_word) # Technique 11: replaces an elongated word with its basic form, unless the word exists in the lexicon\n",
    "#             if len(final_word)>1:\n",
    "#                 final_word = spellCorrection(final_word) # Technique 12: correction of spelling errors\n",
    "#             final_word = lemmatizer.lemmatize(final_word) # Technique 14: lemmatizes words\n",
    "#             final_word = stemmer.stem(final_word) # Technique 15: apply stemming to words\n",
    "\n",
    "## NO POS TAGGING END ###\n",
    "                \n",
    "            onlyOneSentenceTokens.append(final_word)           \n",
    "            finalTokens.append(final_word)\n",
    "\n",
    "         \n",
    "    onlyOneSentence = \" \".join(onlyOneSentenceTokens) # form again the sentence from the list of tokens\n",
    "    #print(onlyOneSentence) # print final sentence\n",
    "\n",
    "    \n",
    "    \"\"\" Write the preprocessed text to file \"\"\"\n",
    "    with open(file, \"a\") as result:\n",
    "        result.write(textID+\"\\t\"+y+\"\\t\"+onlyOneSentence+\"\\n\")\n",
    "        \n",
    "    return finalTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block of codes to tokenize text data. The result does not have the features, only tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aec0f19dc56434aa07a5e3a8dede76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc0adfc61e54ee782b1e6c24bc66858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20631), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f1 = open(\"twitter-2016train-A.txt\",\"r\", encoding=\"utf8\", errors='replace').read()\n",
    "\n",
    "t0 = time()\n",
    "totalSentences = 0\n",
    "totalEmoticons = 0\n",
    "totalSlangs = 0\n",
    "totalSlangsFound = []\n",
    "totalElongated = 0\n",
    "totalMultiExclamationMarks = 0\n",
    "totalMultiQuestionMarks = 0\n",
    "totalMultiStopMarks = 0\n",
    "totalAllCaps = 0\n",
    "\n",
    "for line in tqdm(f1.split('\\n')):\n",
    "    totalSentences += 1\n",
    "    feat = []\n",
    "    columns = line.split('\\t')\n",
    "    columns = [col.strip() for col in columns]\n",
    "\n",
    "    textID = (columns[0])\n",
    "    y = (columns[1])\n",
    "\n",
    "    text = removeUnicode(columns[2]) # Technique 0\n",
    "    #print(text) # print initial text\n",
    "    wordCountBefore = len(re.findall(r'\\w+', text)) # word count of one sentence before preprocess    \n",
    "    #print(\"Words before preprocess: \",wordCountBefore,\"\\n\")\n",
    "    \n",
    "    text = replaceURL(text) # Technique 1\n",
    "    text = replaceAtUser(text) # Technique 1\n",
    "    text = removeHashtagInFrontOfWord(text) # Technique 1\n",
    "\n",
    "    temp_slangs, temp_slangsFound = countSlang(text)\n",
    "#     totalSlangs += temp_slangs # total slangs for all sentences\n",
    "    for word in temp_slangsFound:\n",
    "        totalSlangsFound.append(word) # all the slangs found in all sentences\n",
    "    \n",
    "    text = replaceSlang(text) # Technique 2: replaces slang words and abbreviations with their equivalents\n",
    "    text = replaceContraction(text) # Technique 3: replaces contractions to their equivalents\n",
    "    text = removeNumbers(text) # Technique 4: remove integers from text\n",
    "\n",
    "#     emoticons = countEmoticons(text) # how many emoticons in this sentence\n",
    "#     totalEmoticons += emoticons\n",
    "    \n",
    "    text = removeEmoticons(text) # removes emoticons from text\n",
    "\n",
    "    \n",
    "#     totalAllCaps += countAllCaps(text)\n",
    "\n",
    "    totalMultiExclamationMarks += countMultiExclamationMarks(text) # how many repetitions of exlamation marks in this sentence\n",
    "    totalMultiQuestionMarks += countMultiQuestionMarks(text) # how many repetitions of question marks in this sentence\n",
    "    totalMultiStopMarks += countMultiStopMarks(text) # how many repetitions of stop marks in this sentence\n",
    "    \n",
    "    text = replaceMultiExclamationMark(text) # Technique 5: replaces repetitions of exlamation\n",
    "    text = replaceMultiQuestionMark(text) # Technique 5: replaces repetitions of question marks \n",
    "    text = replaceMultiStopMark(text) # Technique 5: replaces repetitions of stop marks \n",
    "    totalElongated += countElongated(text) # how many elongated words emoticons in this sentence\n",
    "    \n",
    "    tokens = tokenize(text, wordCountBefore, textID, y, 'result-2016train-A.txt')  \n",
    "\n",
    "f2 = open(\"twitter-2016test-A.txt\",\"r\", encoding=\"utf8\", errors='replace').read()\n",
    " \n",
    "for line in tqdm(f2.split('\\n')):\n",
    "    totalSentences += 1\n",
    "    feat = []\n",
    "    columns = line.split('\\t')\n",
    "    columns = [col.strip() for col in columns]\n",
    "\n",
    "    textID = (columns[0])\n",
    "    y = (columns[1])\n",
    "\n",
    "    text = removeUnicode(columns[2]) # Technique 0\n",
    "    #print(text) # print initial text\n",
    "    wordCountBefore = len(re.findall(r'\\w+', text)) # word count of one sentence before preprocess    \n",
    "    #print(\"Words before preprocess: \",wordCountBefore,\"\\n\")\n",
    "    \n",
    "    text = replaceURL(text) # Technique 1\n",
    "    text = replaceAtUser(text) # Technique 1\n",
    "    text = removeHashtagInFrontOfWord(text) # Technique 1\n",
    "\n",
    "    temp_slangs, temp_slangsFound = countSlang(text)\n",
    "#     totalSlangs += temp_slangs # total slangs for all sentences\n",
    "    for word in temp_slangsFound:\n",
    "        totalSlangsFound.append(word) # all the slangs found in all sentences\n",
    "    \n",
    "    text = replaceSlang(text) # Technique 2: replaces slang words and abbreviations with their equivalents\n",
    "    text = replaceContraction(text) # Technique 3: replaces contractions to their equivalents\n",
    "    text = removeNumbers(text) # Technique 4: remove integers from text\n",
    "\n",
    "#     emoticons = countEmoticons(text) # how many emoticons in this sentence\n",
    "#     totalEmoticons += emoticons\n",
    "    \n",
    "    text = removeEmoticons(text) # removes emoticons from text\n",
    "\n",
    "    \n",
    "#     totalAllCaps += countAllCaps(text)\n",
    "\n",
    "    totalMultiExclamationMarks += countMultiExclamationMarks(text) # how many repetitions of exlamation marks in this sentence\n",
    "    totalMultiQuestionMarks += countMultiQuestionMarks(text) # how many repetitions of question marks in this sentence\n",
    "    totalMultiStopMarks += countMultiStopMarks(text) # how many repetitions of stop marks in this sentence\n",
    "    \n",
    "    text = replaceMultiExclamationMark(text) # Technique 5: replaces repetitions of exlamation\n",
    "    text = replaceMultiQuestionMark(text) # Technique 5: replaces repetitions of question marks \n",
    "    text = replaceMultiStopMark(text) # Technique 5: replaces repetitions of stop marks \n",
    "    totalElongated += countElongated(text) # how many elongated words emoticons in this sentence\n",
    "    \n",
    "    tokens = tokenize(text, wordCountBefore, textID, y, 'result-2016test-A.txt')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['result-2016test-A', 'result-2016train-A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block of codes to generate features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189e8d7382f547f48e0477f437e4b4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a45e23cf1b91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mseries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'feat_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin-1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    555\u001b[0m                 \u001b[1;31m# scalar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m                     \u001b[0msetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36msetter\u001b[1;34m(item, v)\u001b[0m\n\u001b[0;32m    487\u001b[0m                     \u001b[1;31m# set the item, possibly having a dtype change\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m                     \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m                     \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m                     \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   5994\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5995\u001b[0m         \"\"\"\n\u001b[1;32m-> 5996\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5997\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    786\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 788\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"copy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mas_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[0;32m    436\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malign_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m             \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    data = pd.read_csv(file+'.txt', sep='\\t',header = None, encoding='latin-1')\n",
    "    data.columns = ['id', 'label', 'tokens']\n",
    "    data.loc[data['label'] == 'positive', 'label']=1\n",
    "    data.loc[data['label'] == 'neutral', 'label']=0\n",
    "    data.loc[data['label'] == 'negative', 'label']=-1\n",
    "    for i, row in tqdm(data.iterrows()):\n",
    "        token = row['tokens'].split(' ')\n",
    "        feature = [np.zeros(200)]\n",
    "        length = 0\n",
    "        for word in token :\n",
    "            # Use oov if word is not in model\n",
    "            if (word in model):\n",
    "                length+=1\n",
    "                feature.append(model[word])\n",
    "        if (length == 0):\n",
    "            series = pd.Series(np.sum(feature, axis = 0)*length)\n",
    "        else:\n",
    "            series = pd.Series(np.sum(feature, axis = 0)/length)\n",
    "        for index, x in series.items():\n",
    "            data.loc[i,'feat_'+str(index)] = x \n",
    "    data.to_csv(file + '.csv', encoding='latin-1', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block of codes to process data generated from the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data into panda dataframe. \n",
    "test = pd.read_csv('result-2016test-A.txt', sep='\\t',header = None,  encoding='latin-1')\n",
    "test.columns = ['id', 'label', 'tokens']\n",
    "# Read in data into panda dataframe. \n",
    "train = pd.read_csv('result-2016train-A.txt', sep='\\t',header = None,  encoding='latin-1')\n",
    "train.columns = ['id', 'label', 'tokens']\n",
    "# Change the labels from String to Integer\n",
    "test.loc[test['label'] == 'positive', 'label']=1\n",
    "test.loc[test['label'] == 'neutral', 'label']=0\n",
    "test.loc[test['label'] == 'negative', 'label']=-1\n",
    "# Change the labels from String to Integer\n",
    "train.loc[train['label'] == 'positive', 'label']=1\n",
    "train.loc[train['label'] == 'neutral', 'label']=0\n",
    "train.loc[train['label'] == 'negative', 'label']=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[train.columns[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[train.columns[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42).fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5261666666666667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6364693907226988"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(SVC()).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(15,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5291666666666667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6602685279433862"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
